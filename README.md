# Тема “Вычисления с помощью Numpy”

## Задание 1
Создайте массив Numpy под названием `a` размером 5x2, то есть состоящий из 5 строк и 2 столбцов. Первый столбец должен содержать числа 1, 2, 3, 3, 1, а второй - числа 6, 8, 11, 10, 7. Будем считать, что каждый столбец - это признак, а строка - наблюдение. Затем найдите среднее значение по каждому признаку, используя метод `mean` массива Numpy. Результат запишите в массив `mean_a`, в нем должно быть 2 элемента.

## Задание 2
Вычислите массив `a_centered`, отняв от значений массива `a` средние значения соответствующих признаков, содержащиеся в массиве `mean_a`. Вычисление должно производиться в одно действие. Получившийся массив должен иметь размер 5x2.

## Задание 3
Найдите скалярное произведение столбцов массива `a_centered`. В результате должна получиться величина `a_centered_spa`. Затем поделите `a_centered_sp` на N-1, где N - число наблюдений.

## Задание 4
Число, которое мы получили в конце задания 3, является ковариацией двух признаков, содержащихся в массиве `a`. В задании 4 мы делили сумму произведений центрированных признаков на N-1, а не на N, поэтому полученная нами величина является несмещенной оценкой ковариации. Подробнее узнать о ковариации можно [здесь](https://studopedia.ru/15_82819_viborochniy-kovar-iatsiya-i-viborochniy-dispersiya.html).

В этом задании проверьте получившееся число, вычислив ковариацию еще одним способом - с помощью функции `np.cov`. В качестве аргумента `m` функция `np.cov` должна принимать транспонированный массив `a`. В получившейся ковариационной матрице (массив Numpy размером 2x2) искомое значение ковариации будет равно элементу в строке с индексом 0 и столбце с индексом 1.

# Тема “Работа с данными в Pandas”

## Задание 1
Импортируйте библиотеку Pandas и дайте ей псевдоним `pd`. Создайте датафрейм `authors` со столбцами `author_id` и `author_name`, в которых соответственно содержатся данные: `[1, 2, 3]` и `['Тургенев', 'Чехов', 'Островский']`. Затем создайте датафрейм `book` со столбцами `author_id`, `book_title` и `price`, в которых соответственно содержатся данные:
- `[1, 1, 1, 2, 2, 3, 3]`
- `['Отцы и дети', 'Рудин', 'Дворянское гнездо', 'Толстый и тонкий', 'Дама с собачкой', 'Гроза', 'Таланты и поклонники']`
- `[500, 400, 300, 350, 450, 600, 200]`

## Задание 2
Получите датафрейм `authors_price`, соединив датафреймы `authors` и `books` по полю `author_id`.

## Задание 3
Создайте датафрейм `top5`, в котором содержатся строки из `authors_price` с пятью самыми дорогими книгами.

## Задание 4
Создайте датафрейм `authors_stat` на основе информации из `authors_price`. В датафрейме `authors_stat` должны быть четыре столбца:
- `author_name`
- `min_price`
- `max_price`
- `mean_price`

В этих столбцах должны содержаться соответственно имя автора, минимальная, максимальная и средняя цена на книги этого автора.

## Задание 5
Создайте новый столбец в датафрейме `authors_price` под названием `cover`, в нем будут располагаться данные о том, какая обложка у данной книги - твердая или мягкая. В этот столбец поместите данные из следующего списка:
- `['твердая', 'мягкая', 'мягкая', 'твердая', 'твердая', 'мягкая', 'мягкая']`

Просмотрите документацию по функции `pd.pivot_table` с помощью вопросительного знака.

Для каждого автора посчитайте суммарную стоимость книг в твердой и мягкой обложке. Используйте для этого функцию `pd.pivot_table`. При этом столбцы должны называться "твердая" и "мягкая", а индексами должны быть фамилии авторов. Пропущенные значения стоимостей заполните нулями, при необходимости загрузите библиотеку Numpy.

Назовите полученный датасет `book_info` и сохраните его в формат pickle под названием `book_info.pkl`. Затем загрузите из этого файла датафрейм и назовите его `book_info2`. Удостоверьтесь, что датафреймы `book_info` и `book_info2` идентичны.


# Тема “Визуализация данных в Matplotlib”

## Задание 1
Загрузите модуль `pyplot` библиотеки `matplotlib` с псевдонимом `plt`, а также библиотеку `numpy` с псевдонимом `np`. Примените магическую функцию `%matplotlib inline` для отображения графиков в Jupyter Notebook и настройки конфигурации ноутбука со значением 'svg' для более четкого отображения графиков. Создайте список под названием `x` с числами 1, 2, 3, 4, 5, 6, 7 и список `y` с числами 3.5, 3.8, 4.2, 4.5, 5, 5.5, 7. С помощью функции `plot` постройте график, соединяющий линиями точки с горизонтальными координатами из списка `x` и вертикальными - из списка `y`. Затем в следующей ячейке постройте диаграмму рассеяния (другие названия - диаграмма разброса, scatter plot).

## Задание 2
С помощью функции `linspace` из библиотеки `Numpy` создайте массив `t` из 51 числа от 0 до 10 включительно. Создайте массив Numpy под названием `f`, содержащий косинусы элементов массива `t`. Постройте линейную диаграмму, используя массив `t` для координат по горизонтали, а массив `f` - для координат по вертикали. Линия графика должна быть зеленого цвета. Выведите название диаграммы - 'График f(t)'. Также добавьте названия для горизонтальной оси - 'Значения t' и для вертикальной - 'Значения f'. Ограничьте график по оси x значениями 0.5 и 9.5, а по оси y - значениями -2.5 и 2.5.

## Задание 3*
С помощью функции `linspace` библиотеки `Numpy` создайте массив `x` из 51 числа от -3 до 3 включительно. Создайте массивы `y1`, `y2`, `y3`, `y4` по следующим формулам:
- `y1 = x**2`
- `y2 = 2 * x + 0.5`
- `y3 = -3 * x - 1.5`
- `y4 = sin(x)`

Используя функцию `subplots` модуля `matplotlib.pyplot`, создайте объект `matplotlib.figure.Figure` с названием `fig` и массив объектов `Axes` под названием `ax`, причем так, чтобы у вас было 4 отдельных графика в сетке, состоящей из двух строк и двух столбцов. В каждом графике массив `x` используется для координат по горизонтали. В левом верхнем графике для координат по вертикали используйте `y1`, в правом верхнем - `y2`, в левом нижнем - `y3`, в правом нижнем - `y4`. Дайте название графикам: 'График y1', 'График y2' и т.д. Для графика в левом верхнем углу установите границы по оси x от -5 до 5. Установите размеры фигуры 8 дюймов по горизонтали и 6 дюймов по вертикали. Вертикальные и горизонтальные зазоры между графиками должны составлять 0.3.

## Задание 4*
В этом задании мы будем работать с датасетом, в котором приведены данные по мошенничеству с кредитными данными: Credit Card Fraud Detection (информация об авторах: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015). Ознакомьтесь с описанием и скачайте датасет `creditcard.csv` с сайта Kaggle.com по ссылке: [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud). Данный датасет является примером несбалансированных данных, так как мошеннические операции с картами встречаются реже обычных. Импортируйте библиотеку `Pandas`, а также используйте для графиков стиль “fivethirtyeight”.

Посчитайте с помощью метода `value_counts` количество наблюдений для каждого значения целевой переменной `Class` и примените к полученным данным метод `plot`, чтобы построить столбчатую диаграмму. Затем постройте такую же диаграмму, используя логарифмический масштаб. На следующем графике постройте две гистограммы по значениям признака `V1` - одну для мошеннических транзакций (`Class` равен 1) и другую - для обычных (`Class` равен 0). Подберите значение аргумента `density` так, чтобы по вертикали графика было расположено не число наблюдений, а плотность распределения. Число бинов должно равняться 20 для обеих гистограмм, а коэффициент `alpha` сделайте равным 0.5, чтобы гистограммы были полупрозрачными и не загораживали друг друга. Создайте легенду с двумя значениями: “Class 0” и “Class 1”. Гистограмма обычных транзакций должна быть серого цвета, а мошеннических - красного. Горизонтальной оси дайте название “V1”.

# Задание на повторение материала
1. Создать одномерный массив `Numpy` под названием `a` из 12 последовательных целых чисел от 12 до 24 невключительно.
2. Создать 5 двумерных массивов разной формы из массива `a`. Не использовать в аргументах метода `reshape` число -1.
3. Создать 5 двумерных массивов разной формы из массива `a`. Использовать в аргументах метода `reshape` число -1 (в трех примерах - для обозначения числа столбцов, в двух - для строк).
4. Можно ли массив `Numpy`, состоящий из одного столбца и 12 строк, назвать одномерным?
5. Создать массив из 3 строк и 4 столбцов, состоящий из случайных чисел с плавающей запятой из нормального распределения со средним, равным 0 и среднеквадратичным отклонением, равным 1.0. Получить из этого массива одномерный массив с таким же атрибутом `size`, как и исходный массив.
6. Создать массив `a`, состоящий из целых чисел, убывающих от 20 до 0 невключительно с интервалом 2.
7. Создать массив `b`, состоящий из 1 строки и 10 столбцов: целых чисел, убывающих от 20 до 1 невключительно с интервалом 2. В чем разница между массивами `a` и `b`?
8. Вертикально соединить массивы `a` и `b`. `a` - двумерный массив из нулей, число строк которого больше 1 и на 1 меньше, чем число строк двумерного массива `b`, состоящего из единиц. Итоговый массив `v` должен иметь атрибут `size`, равный 10.
9. Создать одномерный массив `a`, состоящий из последовательности целых чисел от 0 до 12. Поменять форму этого массива, чтобы получилась матрица `A` (двумерный массив `Numpy`), состоящая из 4 строк и 3 столбцов. Получить матрицу `At` путем транспонирования матрицы `A`. Получить матрицу `B`, умножив матрицу `A` на матрицу `At` с помощью матричного умножения. Какой размер имеет матрица `B`? Получится ли вычислить обратную матрицу для матрицы `B` и почему?
10. Инициализируйте генератор случайных чисел с помощью объекта `seed`, равного 42.
11. Создайте одномерный массив `c`, составленный из последовательности 16-ти случайных равномерно распределенных целых чисел от 0 до 16 невключительно.
12. Поменяйте его форму так, чтобы получилась квадратная матрица `C`. Получите матрицу `D`, поэлементно прибавив матрицу `B` из предыдущего вопроса к матрице `C`, умноженной на 10. Вычислите определитель, ранг и обратную матрицу `D_inv` для `D`.
13. Приравняйте к нулю отрицательные числа в матрице `D_inv`, а положительные - к единице. Убедитесь, что в матрице `D_inv` остались только нули и единицы. С помощью функции `numpy.where`, используя матрицу `D_inv` в качестве маски, а матрицы `B` и `C` - в качестве источников


# Тема: Обучение с учителем

## Задание 1

- Импортируйте библиотеки pandas и numpy.
- Загрузите "Boston House Prices dataset" из встроенных наборов данных библиотеки sklearn.
- Разбейте эти данные на тренировочные (X_train, y_train) и тестовые (X_test, y_test) с помощью функции train_test_split так, чтобы размер тестовой выборки составлял 30% от всех данных, при этом аргумент random_state должен быть равен 42.
- Создайте модель линейной регрессии под названием lr с помощью класса LinearRegression из модуля sklearn.linear_model.
- Обучите модель на тренировочных данных (используйте все признаки) и сделайте предсказание на тестовых.

## Задание 2

- Создайте модель под названием model с помощью класса RandomForestRegressor из модуля sklearn.ensemble.
- Установите аргумент n_estimators равным 1000, max_depth должен быть равен 12, а random_state равен 42.
- Обучите модель на тренировочных данных аналогично тому, как вы обучали модель LinearRegression, но при этом в метод fit вместо датафрейма y_train поставьте y_train.values[:, 0], чтобы получить из датафрейма одномерный массив Numpy, так как для класса RandomForestRegressor предпочтительно использовать массивы вместо датафреймов.
- Сделайте предсказание на тестовых данных и рассчитайте R2. Сравните результат с предыдущим заданием.

## Задание 3

- Вызовите документацию для класса RandomForestRegressor и найдите информацию об атрибуте feature_importances_.
- С помощью этого атрибута найдите сумму всех показателей важности и определите два признака с наибольшей важностью.

## Задание 4

- В этом задании решается задача классификации для датасета Credit Card Fraud Detection. Загрузите датасет creditcard.csv и создайте датафрейм df.
- Используя метод value_counts с аргументом normalize=True, убедитесь, что выборка несбалансирована.
- С помощью метода info проверьте, все ли столбцы содержат числовые данные и нет ли в них пропусков.
- Установите pd.options.display.max_columns = 100 для просмотра всех столбцов датафрейма.
- Просмотрите первые 10 строк датафрейма df.
- Создайте датафрейм X из датафрейма df, исключив столбец Class.
- Создайте объект Series под названием y из столбца Class.
- Разбейте X и y на тренировочный и тестовый наборы данных при помощи функции train_test_split, используя аргументы: test_size=0.3, random_state=100, stratify=y.
- Просмотрите информацию о форме X_train, X_test, y_train и y_test.
- Для поиска по сетке параметров задайте такие параметры:
  ```python
  parameters = [{'n_estimators': [10, 15],
                 'max_features': np.arange(3, 5),
                 'max_depth': np.arange(4, 7)}]

- Создайте модель GridSearchCV со следующими аргументами:
estimator=RandomForestClassifier(random_state=100),
param_grid=parameters,
scoring='roc_auc',
cv=3.
- Обучите модель на тренировочном наборе данных (может занять несколько минут).
- Просмотрите параметры лучшей модели с помощью атрибута best_params_.
- Предскажите вероятности классов с помощью полученной модели и метода predict_proba.
- Из полученного результата (массив Numpy) выберите столбец с индексом 1 (вероятность класса 1) и
запишите в массив y_pred_proba. Из модуля sklearn.metrics импортируйте метрику roc_auc_score.
- Вычислите AUC на тестовых данных и сравните с результатом,полученным на тренировочных данных,
используя в качестве аргументов массивы y_test и y_pred_proba.

# Дополнительные задания:
1. Загрузите датасет Wine из встроенных датасетов sklearn.datasets с помощью функции load_wine в
переменную data.
2. Полученный датасет не является датафреймом. Это структура данных, имеющая ключи
аналогично словарю. Просмотрите тип данных этой структуры данных и создайте список data_keys,
содержащий ее ключи.
3. Просмотрите данные, описание и названия признаков в датасете. Описание нужно вывести в виде
привычного, аккуратно оформленного текста, без обозначений переноса строки, но с самими
переносами и т.д.
© geekbrains.ru 2
4. Сколько классов содержит целевая переменная датасета? Выведите названия классов.
5. На основе данных датасета (они содержатся в двумерном массиве Numpy) и названий признаков
создайте датафрейм под названием X.
6. Выясните размер датафрейма X и установите, имеются ли в нем пропущенные значения.
7. Добавьте в датафрейм поле с классами вин в виде чисел, имеющих тип данных numpy.int64.
Название поля - 'target'.
8. Постройте матрицу корреляций для всех полей X. Дайте полученному датафрейму название
X_corr.
9. Создайте список high_corr из признаков, корреляция которых с полем target по абсолютному
значению превышает 0.5 (причем, само поле target не должно входить в этот список).
10. Удалите из датафрейма X поле с целевой переменной. Для всех признаков, названия которых
содержатся в списке high_corr, вычислите квадрат их значений и добавьте в датафрейм X
соответствующие поля с суффиксом '_2', добавленного к первоначальному названию признака.
Итоговый датафрейм должен содержать все поля, которые, были в нем изначально, а также поля с
признаками из списка high_corr, возведенными в квадрат. Выведите описание полей датафрейма X с
помощью метода describe.

# Тема “Обучение без учителя”

## Задание 1

- Импортируйте библиотеки pandas, numpy и matplotlib.
- Загрузите "Boston House Prices dataset" из встроенных наборов данных библиотеки sklearn.
- Создайте датафреймы X и y из этих данных.
- Разбейте эти датафреймы на тренировочные (X_train, y_train) и тестовые (X_test, y_test) с помощью функции train_test_split так, чтобы размер тестовой выборки составлял 20% от всех данных, при этом аргумент random_state должен быть равен 42.
- Масштабируйте данные с помощью StandardScaler.
- Постройте модель TSNE на тренировочных данных с параметрами: n_components=2, learning_rate=250, random_state=42.
- Постройте диаграмму рассеяния на этих данных.

## Задание 2

- С помощью KMeans разбейте данные из тренировочного набора на 3 кластера, используйте все признаки из датафрейма X_train.
- Параметр max_iter должен быть равен 100, random_state сделайте равным 42.
- Постройте еще раз диаграмму рассеяния на данных, полученных с помощью TSNE, и раскрасьте точки из разных кластеров разными цветами.
- Вычислите средние значения price и CRIM в разных кластерах.

## Задание 3

- Примените модель KMeans, построенную в предыдущем задании, к данным из тестового набора.
- Вычислите средние значения price и CRIM в разных кластерах на тестовых данных.

# Курсовой проект для курса "Python для Data Science"

## Материалы к проекту

- `train.csv`
- `test.csv`

## Задание

Используя данные из `train.csv`, построить модель для предсказания цен на недвижимость (квартиры). С помощью полученной модели предсказать цены для квартир из файла `test.csv`.

## Целевая переменная

- `Price`

## Метрика

- Коэффициент детерминации (R²) - `sklearn.metrics.r2_score`

## Сдача проекта

1. Прислать в раздел Задания Урока 10 ("Вебинар. Консультация по итоговому проекту") ссылку на программу в GitHub (программа должна быть в файле Jupyter Notebook с расширением `.ipynb`). Pull request не нужен, только ссылка на сам скрипт.
2. Приложить файл с названием по образцу `SShirkin_predictions.csv` с предсказанными ценами для квартир из `test.csv` (файл должен содержать два поля: `Id`, `Price`). В файле с предсказаниями должна быть 5001 строка (шапка + 5000 предсказаний).

## Сроки и условия сдачи

- **Дедлайн**: проект нужно сдать в течение 72 часов после начала Урока 10 ("Вебинар. Консультация по итоговому проекту").
- **Условия**:
  - Для успешной сдачи должны быть все предсказания (для 5000 квартир).
  - R² должен быть больше 0.6.
  - При сдаче до дедлайна результат проекта может попасть в топ лучших результатов.
  - Повторная сдача и проверка результатов возможны только при условии предыдущей неуспешной сдачи.
  - Успешный проект нельзя пересдать в целях повышения результата.
  - Проекты, сданные после дедлайна или повторно, не попадают в топ лучших результатов, но можно узнать результат.
  - В качестве итогового результата берется первый успешный результат, последующие успешные результаты не учитываются.

## Примечание

- Все файлы CSV должны содержать названия полей (header - "шапку"), разделитель - запятая. В файлах не должны содержаться индексы из датафрейма.

## Рекомендации для файла с кодом (`.ipynb`)

1. Файл должен содержать заголовки и комментарии.
2. Повторяющиеся операции лучше оформлять в виде функций.
3. Не выводить большое количество строк таблиц (5-10 достаточно).
4. По возможности добавлять графики, описывающие данные (около 3-5).
5. Добавлять только лучшую модель, не включать в код все варианты решения проекта.
6. Скрипт проекта должен отрабатывать от начала и до конца (от загрузки данных до выгрузки предсказаний).
7. Весь проект должен быть в одном скрипте (файл `.ipynb`).
8. При использовании статистик (среднее, медиана и т.д.) в качестве признаков, лучше считать их на трейне и потом на валидационных и тестовых данных не считать статистики заново, а брать их с трейна. Если хватает знаний, можно использовать кросс-валидацию, но для сдачи этого проекта достаточно разбить данные из `train.csv` на train и valid.
9. Проект должен полностью отрабатывать за разумное время (не больше 10 минут), поэтому в финальный вариант лучше не включать GridSearch с перебором большого количества сочетаний параметров.
10. Допускается применение библиотек Python и моделей машинного обучения, которые были в курсе Python для Data Science. Градиентный бустинг изучается в последующих курсах, поэтому в этом проекте его применять не следует. Самая сложная из допустимых моделей - `RandomForestRegressor` из `sklearn`.
